---
title: "K_means_v2"
output: html_document
---


## Nota: recordar que hay que usar el sideeffects INVERSEEEEEE
```{r}
# Establecemos una semilla para obtener siempre los mismos resultados
set.seed(3)
library(tm)
library(factoextra)
library(proxy)
library(ggpubr)

# Cargamos los tados
datos_train <- read.table("datos_train_preprocesado.csv", sep=",", comment.char="",quote = "\"", header=TRUE)

datos_test <- read.table("datos_test_preprocesado.csv", sep=",", comment.char="",quote = "\"", header=TRUE)
# Establecemos la semilla
set.seed(3)

# Nos quedamos con las columnas que nos interesan 
datos_train2 = datos_train[c(2,8,9)]
datos_test2 = datos_test[c(2,8,9)]
head(datos_train2)
```

Los resultados que se obtienen son muy impredecibles. Vamos a obtener los fármacos representados por una única vez. 

```{r}
# km.res <- kmeans(datos_train2, 4)
# 
# 
# fviz_cluster(object = km.res, data = datos_train2, show.clust.cent = TRUE,
#              ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
#   labs(title = "Resultados clustering K-means") +
#   theme_bw() +
#   theme(legend.position = "none")


```

## Arreglamos el conjunto de datos para poder trabajar con lo que nos interesa

```{r}

# https://stackoverflow.com/questions/40003028/extracting-unique-values-from-data-frame-using-r
nombres_farmacos <- unique(datos_train[,1])

print(nombres_farmacos[1:10])

# Creamos una matriz con tantas filas como fármacos, y columnas como datos queramos utilizar. En este caso son 3 columnas porque necesitamos guardar la info de "rating", "sideEffectNumber" y "effectivenessNumber".
datos_procesados <- matrix(ncol=3, nrow=length(nombres_farmacos))

df_farmacos = as.data.frame(nombres_farmacos)
for(i in 0:length(nombres_farmacos)){
  #https://stackoverflow.com/questions/24831580/return-row-of-data-frame-based-on-value-in-a-column-r
  filas_farmaco <- datos_train2[which(datos_train$urlDrugName == nombres_farmacos[i]),]

  mean_rating <- mean(filas_farmaco$rating)
  mean_side_effect <- mean(filas_farmaco$sideEffectsNumber)
  mean_effectiveness <- mean(filas_farmaco$effectivenessNumber)

  datos_procesados[i,] <- c(mean_rating, round(mean_side_effect), round(mean_effectiveness))
}


data_train_procesado <- data.frame(datos_procesados)
rownames(data_train_procesado) <- nombres_farmacos
colnames(data_train_procesado) <- c("rating", "sideEffectNumber", "effectivenessNumber")
```

## K-medias

```{r}
res_clustering <- kmeans(data_train_procesado, 2)

# https://rpubs.com/Joaquin_AR/310338 PASOS SEGUIDOS  

fviz_cluster(object = res_clustering, data = data_train_procesado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, labelsize = 0) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```

Voy a repetir el mismo cálculo, pero en este caso solamente mostrando fármacos "famosos" o significativos, ya que salen muchos fármacos y no se entiende nada, y de esta forma quizá conseguimos quedarnos con los más importantes y entender algo del gráfico resultante.

Así que he copiado la misma función de arriba, pero le he añadido un nuevo parámetro a la función, que representa a partir de cuántas repeticiones de un fármaco consideramos que este es relevante para ponerlo en el clustering. 


De momento  hemos comentado todo esto porque vamos a hacer los cálculos con todos… Esta funcion,  que se queda solo con los que parecen ser más repetidos, la vamos a comentar de momento porque solo la utilizariamos en el caso de que queramos hacer que la funcion sea mas visible pero visualmente y asi poder ver mejor los clusters. De momento, comentada. CUANDO DOCUMENTEMOS MÁS, YA NOS LO PENSAMOS :) 

```{r}

# 
# # https://stackoverflow.com/questions/40003028/extracting-unique-values-from-data-frame-using-r
# nombres_farmacos <- unique(datos_train[,1])
# 
# print(nombres_farmacos[1:10])
# 
# # Creamos una matriz con tantas filas como fármacos, y columnas como datos queramos utilizar. En este caso son 3 columnas porque necesitamos guardar la info de "rating", "sideEffectNumber" y "effectivenessNumber".
# datos_procesados <- matrix(ncol=3, nrow=length(nombres_farmacos))
# 
# for(i in 0:length(nombres_farmacos)){
#   #https://stackoverflow.com/questions/24831580/return-row-of-data-frame-based-on-value-in-a-column-r
#   filas_farmaco <- datos_train2[which(datos_train$urlDrugName == nombres_farmacos[i]),]
#   if (nrow(filas_farmaco)>20 ){
#   mean_rating <- mean(filas_farmaco$rating)
#   mean_side_effect <- mean(filas_farmaco$sideEffectsNumber)
#   mean_effectiveness <- mean(filas_farmaco$effectivenessNumber)}
# 
#   #print(mean_rating)
#   datos_procesados[i,] <- c(mean_rating, round(mean_side_effect), round(mean_effectiveness))
#   
# }
# 
# 
# data_train_procesado <- data.frame(datos_procesados)
# rownames(data_train_procesado) <- nombres_farmacos
# colnames(data_train_procesado) <- c("rating", "sideEffectNumber", "effectivenessNumber")



#res_clustering <- kmeans(data_train_procesado, 5)


```

```{r}
fviz_cluster(object = res_clustering, data = data_train_procesado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```


```{r}

library(factoextra)
datos <- scale(data_train_procesado)
km_clusters <- eclust(x = datos, FUNcluster = "kmeans", k = 6, seed = 123,
                      hc_metric = "euclidean", nstart = 50, graph = FALSE)
fviz_silhouette(sil.obj = km_clusters, print.summary = TRUE, palette = "jco",
                ggtheme = theme_classic()) 

# Media silhouette por cluster
km_clusters$silinfo$clus.avg.widths

```

```{r}
km_clusters <- eclust(x = datos, FUNcluster = "kmeans", k = 8, seed = 123, 
                      hc_metric = "euclidean", nstart = 50, graph = FALSE)
p1 <- fviz_cluster(object = km_clusters, geom = "point", ellipse.type  = "norm",
                   palette = "jco") +
      theme_classic() + theme(legend.position = "none") 

p2 <- fviz_silhouette(sil.obj = km_clusters, print.summary = FALSE,
                      palette = "jco", ggtheme = theme_classic()) +
      theme(legend.position = "none")

ggarrange(p1, p2)
```

Vamos a intentar hacer predict...... lloran2

Primero tenemos que preprocesar test de la misma forma que hemos hecho en train

```{r}

nombres_farmacos <- unique(datos_test[,1])

print(nombres_farmacos[1:10])

# Creamos una matriz con tantas filas como fármacos, y columnas como datos queramos utilizar. En este caso son 3 columnas porque necesitamos guardar la info de "rating", "sideEffectNumber" y "effectivenessNumber".
datos_procesados <- matrix(ncol=3, nrow=length(nombres_farmacos))

df_farmacos_test = as.data.frame(nombres_farmacos)
for(i in 0:length(nombres_farmacos)){
  #https://stackoverflow.com/questions/24831580/return-row-of-data-frame-based-on-value-in-a-column-r
  filas_farmaco <- datos_test2[which(datos_test$urlDrugName == nombres_farmacos[i]),]
  
  mean_rating <- mean(filas_farmaco$rating)
  mean_side_effect <- mean(filas_farmaco$sideEffectsNumber)
  mean_effectiveness <- mean(filas_farmaco$effectivenessNumber)

  #print(mean_rating)
  datos_procesados[i,] <- c(mean_rating, round(mean_side_effect), round(mean_effectiveness))
  
}


data_test_procesado <- data.frame(datos_procesados)
rownames(data_test_procesado) <- nombres_farmacos
colnames(data_test_procesado) <- c("rating", "sideEffectNumber", "effectivenessNumber")


```

Ahora vamos a hacer predict con el clustering que nos ha generado train

```{r}
library(rattle)
#library(caret)
# Hacemos predict del clustering
test_preds <- predict(res_clustering, data_test_procesado)

table(test_preds)
datos_test_carlos = cbind(test_preds)
rownames(datos_test_carlos) = rownames(data_test_procesado)

test_carlos = res_clustering
test_carlos$cluster = datos_test_carlos

fviz_cluster(object = test_carlos, data = data_test_procesado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, labelsize = 0) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")

```


Creo que esto no se puede aplicar pq el clustering no se hace en base a ninguno de los valores........... 
```{r}
# #confusionMatrix(test_preds, my.testLabels )
# 
# pred_test <- predict(res_clustering, data_test_procesado)
# pred_train <- predict(res_clustering, data_train_procesado)
# Etest <- mean(pred_test!=data_test_procesado$effectivenessNumber)
# Etrain <- mean(pred_train!=data_train_procesado$effectivenessNumber)
# 
# cat("-------------------------------\n")
# cat("*MATRIZ DE CONFUSIÓN TEST*\n")
# 
# print(table(pred=pred_test,real=data_test_procesado$effectivenessNumber))
# 
# cat(paste("Error de Test: ",Etest*100," %\n\n"))
# 
# cat("------------------------------------\n")
# cat("*MATRIZ DE CONFUSIÓN TRAIN*\n")
# 
# print(table(pred=pred_train,real=data_train_procesado$effectivenessNumber))
# 
# cat(paste("Error de Train: ",Etrain*100," %\n"))
# cat("------------------------------------")

```


https://stats.stackexchange.com/questions/144616/comparing-k-means-results-to-original-data-how-to-interpret-the-resulting-plots quiza sirve para pintar graficos de estos de cajas chulis


```{r}

```

ahora voy a buscar un farmaco que exista en los dos conjuntos. Esto está MAL porque no estamos cogiendo los indices correctos. creo que tenenos que buscar los indices de data_train_procesado y data_test_procesado. CONTINUAR POR AQUI.

```{r}
## Vamos a comprobar si las predicciones se realizan correctamente.

# Primero buscamos los fármacos que estén en tanto en train como en test, pq son con los que podemos ver si funciona o no el clustering. Vamos a ver si coinciden. 
x = intersect(datos_test$urlDrugName, datos_train$urlDrugName)
head(x)

# Buscamos el indice donde esta el valor
# df[match(item,df$cname),]
indice_train = match('biaxin',datos_train$urlDrugName)
indice_test = match('biaxin',datos_test$urlDrugName)

# Comprobamos mirando en datos_train que el indice es correcto
print(indice_train)
print(indice_test)
# imprimimos cluster que tiene en train 
res_clustering$cluster[indice_train]
# Por ahor todo way

# Comprobamos ahora test
test_carlos$cluster[indice_test]



# nuevo intento

indice_train = match('sarafem',df_farmacos$nombres_farmacos)
indice_test = match('sarafem',df_farmacos_test$nombres_farmacos)

# imprimimos cluster que tiene en train 
res_clustering$cluster[indice_train]
test_carlos$cluster[indice_test]


count = 0
count_na = 0
for(farmaco in x){
  indice_train = match(farmaco,df_farmacos$nombres_farmacos)
  
  indice_test = match(farmaco,df_farmacos_test$nombres_farmacos)
  
  # imprimimos cluster que tiene en train 
  c_train = res_clustering$cluster[indice_train]

  # imprimimos cluster que tiene en test
  c_test = test_carlos$cluster[indice_test]
  
  if ( !is.na(c_train) && !is.na(c_test) && c_train == c_test){
    count = count +1
  }
}

print(count)
print(count/nrow(df_farmacos_test))
#  repasar la funcion de arriba por si tuviera algun fallo........
```

```{r}

predicciones = function(cluster_train, cluster_test) {
  
  count = 0
  count_na = 0
  x = intersect(datos_test$urlDrugName, datos_train$urlDrugName)
  for(farmaco in x){
    indice_train = match(farmaco,df_farmacos$nombres_farmacos)
    indice_test = match(farmaco,df_farmacos_test$nombres_farmacos)
    
    # imprimimos cluster que tiene en train 
    c_train = res_clustering$cluster[indice_train]
  
    # imprimimos cluster que tiene en test
    c_test = test_carlos$cluster[indice_test]
    
    if ( !is.na(c_train) && !is.na(c_test) && c_train == c_test){
      count = count +1
    }
  }  
  count/nrow(df_farmacos_test)
}

```

```{r}
# Probamos la función
res = predicciones(res_clustering, test_carlos)
print(res)

```



A ver, aquí hay que repasar un poquito esto, porque, con el modelo de RandomForest sale que el errorn

```{r}
library("randomForest")


output.forest <- randomForest(as.factor(effectivenessNumber) ~ . , data = data_train_procesado, replace = T,ntree=400)

output.forest

plot(output.forest)

# Predicciones
mod_rf = predict(output.forest, newdata = data_test_procesado[-3], type="response")

# Fallo
y.falladas = sum(mod_rf!=data_test_procesado$effectivenessNumber)/length(data_test_procesado$effectivenessNumber)

print("Error en test:")
print(y.falladas)

```

Segun esto, nos podemos hcer una funcion que te diga en qué rango encontramos el mejor resultado, y con 150 árboles tenemos más que suficiente para alcanzar un 35% de error. Este error en nuestro caso es muy bueno porque tenemos muchisima subjetividad. Para unas personas, poca efectividad tendrá asociado rating 1 y para otras puede ser rating 3. Esto nos afecta mucho los resultados, y por tanto, aumenta el error. 

```{r}

valores_arboles_150 = seq(from=1, to=150, by=1)
valores_arboles_35 = seq(from=1, to=35, by=1)
valores_arboles_15 = seq(from=1, to=15, by=1)
valores_arboles_20 = seq(from=1, to=20, by=1)
valores_arboles_400 = seq(from=1, to=400, by=1)
valores_arboles_500 = seq(from=1, to=500, by=1)

obtener_arboles_optimo = function(x){
  #error_min = 100.0
  arboles_optimo = 1;
  
  for(i in x){
    error_min = 100.0
    set.seed(3)
    
    output.forest <- randomForest(as.factor(effectivenessNumber) ~ . , data = data_train_procesado, replace = T,ntree=i)

    
    # Predicciones
    mod_rf = predict(output.forest, newdata = data_test_procesado[-3], type="response")
    
    # Fallo
    y.falladas = sum(mod_rf!=data_test_procesado$effectivenessNumber)/length(data_test_procesado$effectivenessNumber)
    
    
    if(y.falladas < error_min){
      error_min = y.falladas
      arboles_optimo = i
    }
  }
  
  cat("Numero de árboles óptimo",arboles_optimo, "\n")
  
  cat("Error minimo conseguido",error_min, "\n")
  
}


cat("El error minimo con 150 árboles:\n")
## El error minimo con 150 árboles:
obtener_arboles_optimo(x = valores_arboles_150)


cat("El error minimo con 20 árboles:\n")
## El error minimo con 20 árboles:
obtener_arboles_optimo(x = valores_arboles_20)

cat("El error minimo con 15 árboles:\n")
## El error minimo con 15 árboles:
obtener_arboles_optimo(x = valores_arboles_15)

cat("El error minimo con 400 árboles:\n")
## El error minimo con 400 árboles:
obtener_arboles_optimo(x = valores_arboles_400)


cat("El error minimo con 500 árboles:\n")
## El error minimo con 500 árboles:
obtener_arboles_optimo(x = valores_arboles_500)
```

## Clustering Difuso

INTENTO FUZZY CLUSTERING
```{r}
#install.packages("ppclust")
library(ppclust)
library(cluster)
library(fclust)

 head(data_train_procesado)
datos <- scale(data_train_procesado)
 fuzzy_cluster <- fanny(x = datos, k = 3, metric = "euclidean", stand = FALSE)
head(fuzzy_cluster$membership)
# fuzzy_cluster$coeff
# head(fuzzy_cluster$clustering)
fviz_cluster(object = fuzzy_cluster, repel = TRUE, ellipse.type = "norm", pallete = "jco") + theme_bw() + labs(title = "Fuzzy Cluster plot")

```

## K - MEDIOIDES CLUSTERING

```{r}
fviz_nbclust(x = datos, FUNcluster = kmeans, method = "silhouette", k.max = 15) +
  labs(title = "Número óptimo de clusters")
```


Nuevo método en el que agrupamos las observaciones en K clusters, siendo este valor K preestablecido. En este caso, cada cluster está representado por una observación presente en el cluster. Mediante este método hacemos uso de medoids, consiguiendo que sea menos afectado por ruido.

Para su resolución usaremos el alogitmo PAM (Partitioning Around Medoids). Este minimiza la suma de las diferencias de cada observación respecto a su medoid.

```{r}

# Vemos cosas
km_clusters <- eclust(x = datos, FUNcluster = "pam", k = 6, seed = 123,
                      hc_metric = "wss",  graph = FALSE)
fviz_silhouette(sil.obj = km_clusters, print.summary = TRUE, palette = "jco",
                ggtheme = theme_classic()) 

# Escalamos antes de aplicar el clustering
datos <- scale(data_train_procesado)

# Identificamos el número optimo de clusters
fviz_nbclust(x = datos, FUNcluster = pam, method = "wss", k.max = 15, diss = dist(datos, method = "manhattan"))


# Vemos que a partir de 6 clusters parece estabilizarse, por tanto, k=6
set.seed(123)
pam_clusters <- pam(x = datos, k = 6, metric = "manhattan")
#pam_clusters
# Observaciones que finalmente se han seleccionado
#pam_clusters$medoids
# Cluster al que se ha asignado cada observación
#pam_clusters$clustering

fviz_cluster(object = pam_clusters, data = datos, ellipse.type = "t", repel = TRUE) +
  theme_bw() +
  labs(title = "Resultados clustering PAM") +
  theme(legend.position = "none")


# Calculamos test


test_preds <- predict(pam_clusters, data_test_procesado)

table(test_preds)
datos_test_carlos = cbind(test_preds)
rownames(datos_test_carlos) = rownames(data_test_procesado)

test_carlos = res_clustering
test_carlos$cluster = datos_test_carlos



```

Como podemos observar, no hay centroides. Ahora vamos a resaltar las observaciones que actúan como medoids.

```{r}

# Calculamos el PCA y extraemos las proyecciones almacenadas en el elemento x.
medoids <- prcomp(datos)$x

# Se seleccionan únicamente las proyecciones de las observaciones que son medoids
medoids <- medoids[rownames(pam_clusters$medoids), c("PC1", "PC2")]
medoids <- as.data.frame(medoids)

# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")

# Creación del gráfico
fviz_cluster(object = pam_clusters, data = datos, ellipse.type = "t",
             repel = TRUE) +
  theme_bw() +
  # Se resaltan las observaciones que actúan como medoids
  geom_point(data = medoids, color = "firebrick", size = 2) +
  labs(title = "Resultados clustering PAM") +
  theme(legend.position = "none")
```

## Hierarchical clustering

Hierarchical clustering es una alternativa a los métodos de partitioning clustering que no requiere que se pre-especifique el número de clusters. Se pueden seguir dos estrategias, aglomerativa o divisiva, en este caso vamos a usar la primera.

En el método aglomerativo el agrupamiento se inicia en el base del árbol, siendo cada observación un cluster individual. Entonces los cluster se van combinando conforme la estructura crece hasta unirse en única rama central.

```{r}

# Vemos cosas
km_clusters <- eclust(x = datos, FUNcluster = "hclust", k = 6, seed = 123,
                      hc_metric = "euclidean",  graph = FALSE)
fviz_silhouette(sil.obj = km_clusters, print.summary = TRUE, palette = "jco",
                ggtheme = theme_classic()) 


# Verificar el árbol

# Evaluamos hasta que punto la estructura refleja las distancias originales entre observaciones. Para ello usamos el coeficiente de correlación entre las distancias cophenetic de la altura de los nodos y la matriz de distancias original.

# Cuanto más cercano al 1 mejor. Valores superiores al 0.75 se consideran buenos.

datos <- scale(data_train_procesado)

# Matriz de distancias euclídeas
mat_dist <- dist(x = datos, method = "euclidean")
# Dendrogramas con linkage complete y average
hc_euclidea_complete <- hclust(d = mat_dist, method = "complete")
hc_euclidea_average  <- hclust(d = mat_dist, method = "average")

cor(x = mat_dist, cophenetic(hc_euclidea_complete)) # 0.7864
cor(x = mat_dist, cophenetic(hc_euclidea_average)) # 0.9374272

```

Además, debemos de poder identificar el número de clusters creados y que observaciones forman parte de cada uno. Si realizamos un corte horizontal en el dendrograma, el número de ramas que lo sobrepasan se corresponden con el número de clusters.

Usamos como altura de corte el valor K de K-means. K=6

```{r}
# Cortar el árbol para generar los clusters

library(factoextra)
datos <- scale(data_train_procesado)
set.seed(101)

hc_euclidea_completo <- hclust(d = dist(x = datos, method = "euclidean"), method = "complete")

fviz_dend(x = hc_euclidea_completo, k = 6, cex = 0.6) +
  geom_hline(yintercept = 5.5, linetype = "dashed") +
  labs(title = "Herarchical clustering",
       subtitle = "Distancia euclídea, Lincage complete, K=6")
```
```{r}

require("igraph")
set.seed(5665)
fviz_dend(x = hc_euclidea_completo,
          k = 6,
          color_labels_by_k = TRUE,
          cex = 0.8,
          type = "phylogenic",
          repel = TRUE, labelsize=0)
```

Otra forma de representar sería combinando con una reducción de dimensionalidad por PCA. Primero, se calculan los componentes principales y después se representan empleando las dos primeras componentes. Finalmente, se colorean los clusters mediante elipses.

```{r}
fviz_cluster(object = list(data=datos, cluster=cutree(hc_euclidea_completo, k=6)),
             ellipse.type = "convex", repel = TRUE, show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "Hierarchical clustering + Proyección PCA",
       subtitle = "Distancia euclídea, Lincage complete, K=6") +
  theme_bw() +
  theme(legend.position = "bottom")
```


## Density based clustering (DBSCAN)

```{r}
library(factoextra)
datos <- data_train_procesado
set.seed(321)
km_clusters <- kmeans(x = datos, centers = 6, nstart = 50)
fviz_cluster(object = km_clusters, data = datos, repel = TRUE, ellipse = FALSE,
             show.clust.cent = FALSE, pallete = "jco") +
  theme_bw() +
  theme(legend.position = "none")
```

```{r}
library(factoextra)


datos <- data_train_procesado
set.seed(321)
km_clusters <- kmeans(x = datos, centers = 6, nstart = 50)
fviz_cluster(object = km_clusters, data = datos, geom = "point", repel = TRUE, ellipse = FALSE,
             show.clust.cent = FALSE, pallete = "jco") +
  theme_bw() +
  theme(legend.position = "none")
```





































