---
title: "KNN"
output:
  pdf_document: default
---

En esta sección vamos a realizar técnicas de clasificación basada en instancias. Concretamente el de vecinos más cercanos (KNN). Esta técnica es puramente predictiva, por lo que nos proporciona ninguna información acerca de dependecia de variables. Dado un texto, busca en el conjunto de entrenamiento los items que tienen un mayor parentesco del que estamos tratando de deducir y copia su clase. La clase que vamos a intentar reducir es **ratingLabel** que es la columna preprocesada de **rating**. Nos hubiera gustado probar y comentar esta técnica tratando de clasificar otras clases, pero no tenemos tiempo suficiente como para ello.

Esta técnica es buena cuando el conjunto de items se encuentran disjuntos entre sí debido a sus clases. Aunque el clustering es una técnica no supervisada en la que de antemano no teníamos etiquetas concretas que clasificar, nos dimos cuenta de que no era una opción que nos aportara mucho debido a este hecho (la nube de puntos no se encontraba disjuntas en grupos claros). Por lo que de antemano pensamos que no va a devolvernos unas predicciones muy prometedoras.

Antes de comenzar, destacar que probamos con variables numéricas antes de trabajar con los textos. Como nuestro dataset no tiene variables continuas, probamos con variables discretas. Tal y como se dice en la teoría de esta asignatura, este algoritmo no trabaja bien con este tipo de valores numéricos debido a que el cálculo de la distancias hacía que hubiera una gran cantidad de distancias iguales e items superpuestos en un plano bidimensional. En definitiva, tuvimos problemas a la hora de procesar el algoritmo (concretamente "too many ties in KNN", debido a lo comentado anteriormente). Así que nos pusimos a trabajar con cálculos de distancias en los comentarios, lo cual es más apropiado para esta técnica debido a que no tenemos valores continuos como tal en nuestro dataset.

Nos encontramos con un problema que tuvimos en la implementación de este algoritmo y el cual nos llevó mucho tiempo detectar y solucionar. Básicamente, nosotros estamos trabajando con dos dataset ya diferenciados desde el comienzo; el train y el test. Para esta técnica debemos de calcular la matriz de términos, si lo hacemos de los dos dataset por separado, tenemos el problema de que ambas matrices de términos no tienen exactamente el mismo formato (las columnas pueden tener diferentes palabras). Entonces, teníamos errores de dimensiones al aplicar el algoritmo, ya que obteníamos un clasificador que no era apto para predecir el test.

La solución, calcular la matriz de términos de un dataset, una vez realizados todos los cálculos y preparación de la matriz de términos que explicaremos más adelante, separamos esa misma matriz de términos (un 70% para el train y un 30% para el test). De esta forma nos aseguramos de que ambas matrices tienen exactamente las mismas columnas y los mismos términos y ya podemos trabajar correctamente con el algoritmo KNN.

Dicho esto, vamos a comenzar. Primero cargamos los datos.

```{r, eval=FALSE}
datos_preprocesados <- read.csv(file="datos/datos_train_preprocesado.csv")
View(datos_preprocesados)

library(class)
library("tm")
```

El siguiente paso consiste en obtener el Corpus como hemos hecho en técnicas anteriores. Primero vamos a hacerlo sobre los comentarios acerca de los beneficios de los medicamentos. Acto seguido, obtenemos la matriz de términos correspondiente con sus vectores de pesos.

Para que el algoritmo funcione, es importante que esa matriz sea de tipo *matrix*. Una vez hecho esto, podemos separar la matriz en el conjunto de entrenamiento y prueba. Obtenemos el clasificador y predecimos solo el error del test porque la función obtiene el clasificador y la predicción en la misma llamada, por lo que para calcular el error de entrenamiento tendríamos que recalcular el mismo clasificador y nos parecía ineficiente.

Como ya hemos mencionado anteriormente, KNN selecciona los **k** items del conjunto de entrenamiento más parecidos al que estamos deduciendo (calculando este parentesco con distancia euclidea). Acto seguido, el algoritmo deduce la clase del item siendo la clase más frecuente de entre sus vecinos.

Por consiguiente, el valor de **k** es muy importante a la hora de realizar el clasificador. Un valor muy alto puede producir sobreaprendizaje y un valor muy bajo puede producir un aumento significativo en la tasa de error, por lo que probaremos con distintos valores. 

```{r, eval=FALSE}
# PAra train KNN
corpus = tm::Corpus(tm::VectorSource(datos_preprocesados$benefits_preprocesado))
tdm <- tm::DocumentTermMatrix(corpus)

# Transform dtm to matrix and then back into a data frame for modeling
mat.df <- as.data.frame(data.matrix(tdm), stringsAsfactors = FALSE)

# Column bind category (known classification)
mat.df <- cbind(mat.df, datos_preprocesados$ratingLabel, row.names = NULL)

colnames(mat.df)[ncol(mat.df)] <- "ratingLabel"

# Classifier variable based on the personality typ
cl <- mat.df[,"ratingLabel"]

# Create model data and remove “type”
modeldata <- mat.df[,!colnames(mat.df) %in% "ratingLabel"]

train <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .70))
test <- (1:nrow(mat.df))[- train]
```

Para **k=1**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=1) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)

```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/benefits_k1.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con beneficios (k=1).}
    \label{fig:KNN:benefitsK1}
\end{figure}

Para K=1 vemos que obtenemos de por sí una tasa de precisión razonable a sabiendas de que no iba a funcionar muy bien. Es decir, clasifica bien el ratingLabel en más de la mitad de las ocasiones.

Como podemos observar en la matriz de confusión, tiene un error alrededor del 40% para el conjunto del test. Como nos temíamos, la técnica no es muy apropiada para nuestro problema debido a la disposición del los textos. Sin embargo, vamos a probar con valores distintos de k para intentar mejorar hasta cierto punto el desempeño del algoritmo. 

Para **k=3**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=3) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)

```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/benefits_k3.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con beneficios (k=3).}
    \label{fig:KNN:benefitsK3}
\end{figure}

Para k=3 el cambio no es significativo, incluso empeora un poco.

Para **k=5**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=5) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)

```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/benefits_k5.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con beneficios (k=5).}
    \label{fig:KNN:benefitsK5}
\end{figure}

Para k=5 tenemos el mejor clasificador hasta el momento, aunque vemos que se mantiene bastante estable, sin cambios realmente significativos.

Para **k=10**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=10) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)
```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/benefits_k10.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con beneficios (k=10).}
    \label{fig:KNN:benefitsK10}
\end{figure}

En este punto, el clasificador tiene un bajón de un 6% en la precisión. Vemos que siempre tenemos más falsos positivos que negativos. Es decir, el clasificador es positivo y suele dar más ratings altos que bajos cuando se equivoca.

Para **k=15**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=15) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)
```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/benefits_k15.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con beneficios (k=15).}
    \label{fig:KNN:benefitsK15}
\end{figure}

Para **k=30**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=30) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)
```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/benefits_k30.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con beneficios (k=30).}
    \label{fig:KNN:benefitsK30}
\end{figure}

Para **k=50**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=50) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)
```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/benefits_k50.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con beneficios (k=50).}
    \label{fig:KNN:benefitsK50}
\end{figure}

Como podemos observar, el modelo mantiene una precisión estable. Sin embargo a partir del valor 5 para **k** cuanto más vecinos, peor es el modelo. Podemos verlo de una forma más sencilla en la siguiente gráfica.

```{r, eval=FALSE}
valores_k <- c(1,3,5,10,15,30,50)
precision <- c(60.79484,60.47261,60.90226,54.24275,47.79807,34.80129,29.32331)

plot(x=valores_k,y=precision, xlab = "Valores de K", ylab="Precisión(%)", ylim = c(0,100), type="o", col="green")
title("Precisión del modelo KNN en función del número de vecinos(K)")


```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/grafico_benefits.png}
    \caption{Gráfico del progreso del modelo KNN en función del valor de k.}
    \label{fig:KNN:grafica}
\end{figure}

Hacemos lo mismo para los comentarios de efectos secundarios que tenemos disponibles en nuestro dataset.

```{r, eval=FALSE}

# Para train KNN
corpus = tm::Corpus(tm::VectorSource(datos_preprocesados$effects_preprocesado))
tdm <- tm::DocumentTermMatrix(corpus)

# Transform dtm to matrix and then back into a data frame for modeling
mat.df <- as.data.frame(data.matrix(tdm), stringsAsfactors = FALSE)

# Column bind category (known classification)
mat.df <- cbind(mat.df, datos_preprocesados$ratingLabel, row.names = NULL)

colnames(mat.df)[ncol(mat.df)] <- "ratingLabel"

# Classifier variable based on the personality typ
cl <- mat.df[,"ratingLabel"]

# Create model data and remove “type”
modeldata <- mat.df[,!colnames(mat.df) %in% "ratingLabel"]

train <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .70))
test <- (1:nrow(mat.df))[- train]
```

Para **k=1**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=1) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)

```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/effects_k1.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con efectos secundarios (k=1).}
    \label{fig:KNN:effectsK1}
\end{figure}

Para **k=3**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=3) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)

```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/effects_k3.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con efectos secundarios (k=3).}
    \label{fig:KNN:effectsK3}
\end{figure}

Para **k=5**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=5) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)

```

Parece que este vuelve a ser el mejor valor para k en la construcción del modelo.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/effects_k5.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con efectos secundarios (k=5).}
    \label{fig:KNN:effectsK5}
\end{figure}

Para **k=15**:

```{r, eval=FALSE}
knn.pred <- knn(modeldata[train,], modeldata[test,], cl[train], k=15) 

conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat

(accuracy <- sum(diag(conf.mat))/length(test) * 100)
```

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figuras/KNN/effects_k15.png}
    \caption{Matriz de confusión y tasa de precisión en modelo KNN con efectos secundarios (k=15).}
    \label{fig:KNN:effectsK15}
\end{figure}

Es curiosos que en este último caso solo tiene falsos positivos, no negativos. 

Vemos que ocurre algo similar que con los comentarios de beneficios, solo que con una tasa de precisión más alta (solo se equivocaría 1 de cada 5 predicciones aproximadamente). Por algún motivo, es capaz de predecir mejor este tipo de comentarios sobre efectos secundarios. Suponemos que porque la nube de puntos es más apropiada en esta ocasión para esta técnica. 

Llegamos a la conclusión en ambos tipo de comentarios que los valores apropiados para k deben ser bajos (valor de 5), por los datos mostrados anteriormente.