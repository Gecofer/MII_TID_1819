---
title: "clustering"
author: "Alejandro Campoy Nieves"
date: "25 de enero de 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Clustering sobre datos de texto

Vamos a ver cómo podemos aplicar técnicas de clustering a las distintas columnas de texto de nuestro dataset. Para ello vamos a explicar en mayor detalle el proceso seguido en la columna Benefits Review preprocesada y luego mostraremos los resultados con los otros textos de una forma más directa.

### Clustering de Benefits Review

Lo primero que debemos hacer es cargar el dataset preprocesado que hemos creado en esta práctica. Formamos el corpus correspondiente a la columna de beneficios que estamos analizando.

```{r, warning=FALSE}
# Cargamos los tados
datos_train <- read.table("datos/datos_train_preprocesado.csv", sep=",", comment.char="",quote = "\"", header=TRUE)
# Establecemos la semilla
set.seed(3)
corpus = tm::Corpus(tm::VectorSource(datos_train$benefits_preprocesad))

```

Como ya hemos hecho en alguna ocasión a lo largo de esta práctica, vamos a crear una matriz de términos en la que cada fila es un documento o comentario de beneficios y cada columna es un término o palabra que aparece en los textos.

```{r, warning=FALSE}
tdm <- tm::DocumentTermMatrix(corpus)
tdm.tfidf <- tm::weightTfIdf(tdm)
tfidf.matrix <- as.matrix(tdm.tfidf)
```

Para poder trabajar con clustering necesitamos trabajar con valores numéricos. Es por ello que hemos utilizado de nuevo TFIDF (esta vez hemos usado una función a de más alto nivel, sin entrar en detalles). Necesitamos tener estos datos como matriz para poder continuar con los siguientes pasos.


```{r, warning=FALSE}
tfidf.matrix[1:20,1:20]

```

Esta es una muestra de la esquina superior izquierda de la matriz para ver el formato explicado anteriormente. Los valores son pesos normalizados de los términos en cada uno de los comentarios.

En una de las técnicas que veremos más adelante necesitamos las 100 palabras con mayor peso. La forma que se nos ocurrió de hacerlo consistía en hacer la sumatoria de los pesos por columnas y después ordenarlas en función del valor. El problema reside en que no podemos perder el número de filas que tenemos (la forma de la matriz), cosa que ocurre si hacemos la sumatoria por columnas. Entonces se nos ocurrió lo siguiente:

```{r, warning=FALSE}

#Hacemos la sumatoria de columnas para averiguar el peso total de cada término.
v <- c()
for(i in 1:length(tfidf.matrix[,1])){
  v[i] <- sum(tfidf.matrix[,i])
}

# Averiguamos el orden de indices de los pesos de mayor a menor y nos quedamos con los 100 mas grandes.
indices <- order(v,decreasing = TRUE)[1:100]

# Nos creamos una submatriz de la original con los términos mas relevantes.
tfidf.matrix100 <- tfidf.matrix[,indices]
tfidf.matrix100 <- t(tfidf.matrix100)
```

En definitiva, en lugar de ordenar la matriz lo que obtenemos es una lista de los indices en la posición que deberían estar para que estuvieran ordenados sus pesos de mayor a menor. Después solo tenemos que quedarnos con los 100 primeros índices (los 100 términos con mayor peso) y generar una submatriz de la matriz original con las columnas que corresponden a esos índices.

El siguiente paso sería calcular la matriz de distancias. Este es el proceso que consume más tiempo de todo el proceos de clustering por lo que debemos ser pacientes.


```{r, warning=FALSE}

# Calculamos la distancia de cada término en cada documento
dist.matrix = proxy::dist(tfidf.matrix, method = "cosine")
dist.matrix100 = proxy::dist(tfidf.matrix100, method = "cosine")
```

Ya tenemos los datos sobre los comentarios de los beneficios de los medicamentos listos para ser utilizados en la técnica de clustering. Principalmente hemos creado tres estrategias para generar los clusters como se puede observar a continuación.

```{r, warning=FALSE}
#------------------------------------------------------------------------------------------------#

### FUNCIONES: QUEREMOS HACER CLUSTERING EN FUNCIÓN DEL NÚMERO DE CLUSTERS QUE DESEEMOS.

#------------------------------------------------------------------------------------------------#

clustering.kmeans <- function(centers=5){
  # Calulamos el cluster
  cluster <- kmeans(tfidf.matrix, centers) 
  
  # Para el de 100 terminos más frecuentes
  cluster100 <- kmeans(tfidf.matrix100, centers) 
  
  # Pintamos la nube de puntos perteneciente a cada uno de los cluster (cada color es un cluster)
  points <- cmdscale(t(dist.matrix), k = 2) 
  palette <- colorspace::diverge_hcl(centers) # Creating a color palette 
  
  plot(points, main = 'K-Means clustering', col = as.factor(cluster$cluster), 
       mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
       xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
  
}

#------------------------------------------------------------------------------------------------#

clustering.hierarchial <- function(centers=5){
  # Calulamos el cluster
  cluster <- hclust(dist.matrix, method = "ward.D2") 
  
  # Para el de 100
  cluster100 <- hclust(dist.matrix100, method = "ward.D2")
  
  # Pintamos la nube de puntos de cada uno de ellos (Cada color representa un cluster distinto)
  points <- cmdscale(dist.matrix, k = 2) 
  palette <- colorspace::diverge_hcl(centers) # Creating a color palette 
  previous.par <- par(mfrow=c(1,2), mar = rep(1.5, 4))
  
  plot(points, main = 'Hierarchical clustering', col = as.factor(cutree(cluster, k = centers)), 
       mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0),  
       xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
  
  # Para el de 100
  plot(cluster100, cex=0.9, hang=-1)
  rect.hclust(cluster100, centers, border=rainbow(centers))
  
}

#------------------------------------------------------------------------------------------------#

clustering.dbscan <- function(centers=5){
  # Calulamos el cluster
  cluster <- dbscan::hdbscan(dist.matrix, minPts = 10)
  
  # Pintamos la nube de puntos de cada uno de ellos (Cada color representa un cluster distinto)
  points <- cmdscale(dist.matrix, k = 2) 
  palette <- colorspace::diverge_hcl(centers) # Creating a color palette 
  
  plot(points, main = 'Density-based clustering', col = as.factor(cluster$cluster), 
       mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
       xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
}
#------------------------------------------------------------------------------------------------#

```

Cada función utilizará por defecto 5 centroides en los que clasificar los diferentes términos. Si se desea cambiar este hecho es posible simplemente especificando el número de centros por medio de un argumento al llamar la función. Vamos a comentar el proceso de cada uno de ellos y los datos obtenidos.

```{r, warning=FALSE}
# Primera técnica
clustering.kmeans()

```

```{r, warning=FALSE}
#Segunda técnica
clustering.hierarchial()

```

```{r, warning=FALSE}
# Tercera técnica
clustering.dbscan()

```

Como podemos ver en las diferentes técnicas, tenemos la misma nube de puntos en la que cada punto representa una palabra. Su color indica el cluster al que pertenece. Consideramos que los resultados más llamativos se encuentran en la segunda técnica, en la que se puede apreciar los clústers más diferenciados en el espacio.

Por ello, probamos a hacer más cosas en el mismo y obtuvimos adicionalmente el dendograma de las 100 palabras con mayor peso entre todos los documentos. Los recuadros de colores indican cada uno de los cluesters y en esta podemos ver de una forma más clara las agrupaciones realizadas durante el proceso (se se amplía el dendograma se pueden ver mejor las palabras). Hace una clasificación que consideramos lógica y con correlación a los comentarios que pueden hacer las personas sobre los beneficios de los medicamentos. 

Intentamos también realizar un word cloud en función de estos clústers, aunque de momento no hemos tenido éxito en ese aspecto.



