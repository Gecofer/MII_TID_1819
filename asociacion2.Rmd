```{r}
library("tm")
library("arules")
library("robustbase")
library("plotly")
library("arulesViz")
```

Primero leemos los datos.

```{r}
datos_train <- read.table("datos/drugLibTrain_raw.tsv", sep="\t", comment.char="",
                          quote = "\"", header=TRUE)
# Eliminar columnas  para el ID en el train
datos_train = datos_train[-c(1,3)]
```



```{r}
datos_train$sideEffectsNumber[datos_train$sideEffects == "Extremely Severe Side Effects"] <- 5
datos_train$sideEffectsNumber[datos_train$sideEffects == "Severe Side Effects"] <- 4
datos_train$sideEffectsNumber[datos_train$sideEffects == "Moderate Side Effects"] <- 3
datos_train$sideEffectsNumber[datos_train$sideEffects == "Mild Side Effects"] <- 2
datos_train$sideEffectsNumber[datos_train$sideEffects == "No Side Effects"] <- 1

head(datos_train$sideEffectsNumber, 10)

#corpus_train <- corpus(datos_train)

```

En este paso, creamos un corpus de la columna "benefitsReview" para poder hacer uso de la libreria "tm". En la creación de dicho corpus cambiamos las palabras a minúcsculas, quitamos los signos de puntuación, las palabras comunes que no nos aportan nada (stopwords) y los espacios en blanco. Estos pasos son necesarios ya que si tomamos el corpus sin modificacion encontraremos palabras que no nos aportan información.

Buscamos encontrar más información de los comentarios, como por ejemplo, como se relacionan, de que hablan, etc.

```{r}
#benefits_review_data = as.vector(datos_train$benefitsReview)
benefits_corpus = Corpus(VectorSource(datos_train$benefitsReview))
benefits_corpus <- tm_map(benefits_corpus, content_transformer(tolower))
benefits_corpus <- tm_map(benefits_corpus, content_transformer(removePunctuation))
benefits_corpus <- tm_map(benefits_corpus, content_transformer(removeWords), stopwords("english"))
benefits_corpus <- tm_map(benefits_corpus, stripWhitespace)
```

Ahora mismo el dataset es de tipo categórico pero nosotros los necesitamos de tipo string. Por tanto, todos los datos de la columna los vamos a transformar a tipo string. Entonces todos los datos de la columna los vamos a transformar a string de palabras distintas separados por espacios.

Una vez que obtenemos las palabras en string, algunas de estas se quedan como palabras vacias, para ello utilizamos la función. Finalmente, ya podemos obtener los elementos de tipo "transactions".

```{r}
items <- strsplit(as.character(benefits_corpus$content), " ")
# Para eliminar las cadenas vacías --> https://stackoverflow.com/questions/24178854/remove-blanks-from-strsplit-in-r
items <- lapply(items, function(x){x[!x ==""]})

transactions <- as(items,"transactions")

```

Como ya tenemos los elementos de tipo "transactions", podemos aplicar la técnica para obtener las reglas de asosiación. Vamos a utilizar el algoritmo "apriori" visto en clase.

El primer parámetro que encontramos en la función se corresponde con los datos que le proporcionamos y el segundo, un listado de parámetros específicos. En cuanto a estos, el primero es el umbral para el soporte, el segundo el umbral de la confianza, el tercero el target para indicar que buscamos reglas de asociación y en el cuarto indicamos que como mínimo empecemos con itemset de tamaño 2.

Ordenamos por "confidence" a vista de mostrar posteriormente los resultados más importantes. Entonces mostraremos las reglas que tienen más confianza y mostraremos graficamente el resultado obtenido.

```{r}
rules <- apriori(transactions, parameter = list(sup = 0.001, conf = 0.7, target="rules", minlen=1))
#rules <- sort(rules, by="support")
top.rules.confidence <- sort(rules, decreasing = TRUE, na.last = NA, by = "confidence")

# Si no funciona inspect --> https://stackoverflow.com/questions/18934098/r-error-with-inspect-function
detach(package:tm, unload=TRUE) 
inspect(head(top.rules.confidence,100))

plot(top.rules.confidence, method="graph")

# Cuando tenemos una regla con una única palabra en el antecedente y en el consecuente deben de ir unidas en el texto
```

