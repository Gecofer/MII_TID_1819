---
title: "K_means_v2"
output: html_document
---

```{r}
library(tm)
library(factoextra)
library(proxy)
library(ggpubr)

# Cargamos los tados
datos_train <- read.table("datos_train_preprocesado.csv", sep=",", comment.char="",quote = "\"", header=TRUE)

datos_test <- read.table("datos_test_preprocesado.csv", sep=",", comment.char="",quote = "\"", header=TRUE)
# Establecemos la semilla
set.seed(3)

# Nos quedamos con las columnas que nos interesan 
datos_train2 = datos_train[c(2,8,9)]
datos_test2 = datos_test[c(2,8,9)]
head(datos_train2)
```

Los resultados que se obtienen son muy impredecibles. Vamos a obtener los fármacos representados por una única vez. 
```{r}
km.res <- kmeans(datos_train2, 4)



fviz_cluster(object = km.res, data = datos_train2, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")



```


```{r}

# https://stackoverflow.com/questions/40003028/extracting-unique-values-from-data-frame-using-r
nombres_farmacos <- unique(datos_train[,1])

print(nombres_farmacos[1:10])

# Creamos una matriz con tantas filas como fármacos, y columnas como datos queramos utilizar. En este caso son 3 columnas porque necesitamos guardar la info de "rating", "sideEffectNumber" y "effectivenessNumber".
datos_procesados <- matrix(ncol=3, nrow=length(nombres_farmacos))

for(i in 0:length(nombres_farmacos)){
  #https://stackoverflow.com/questions/24831580/return-row-of-data-frame-based-on-value-in-a-column-r
  filas_farmaco <- datos_train2[which(datos_train$urlDrugName == nombres_farmacos[i]),]

  mean_rating <- mean(filas_farmaco$rating)
  mean_side_effect <- mean(filas_farmaco$sideEffectsNumber)
  mean_effectiveness <- mean(filas_farmaco$effectivenessNumber)

  #print(mean_rating)
  datos_procesados[i,] <- c(mean_rating, round(mean_side_effect)*(-1), round(mean_effectiveness))
  
}


data_train_procesado <- data.frame(datos_procesados)
rownames(data_train_procesado) <- nombres_farmacos
colnames(data_train_procesado) <- c("rating", "sideEffectNumber", "effectivenessNumber")
```

```{r}
res_clustering <- kmeans(data_train_procesado, 5)

# https://rpubs.com/Joaquin_AR/310338 PASOS SEGUIDOS  

fviz_cluster(object = res_clustering, data = data_train_procesado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, labelsize = 0) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```
Voy a repetir el mismo cálculo, pero en este caso solamente mostrando fármacos "famosos" o significativos, ya que salen muchos fármacos y no se entiende nada, y de esta forma quizá conseguimos quedarnos con los más importantes y entender algo del gráfico resultante.

Así que he copiado la misma función de arriba, pero le he añadido un nuevo parámetro a la función, que representa a partir de cuántas repeticiones de un fármaco consideramos que este es relevante para ponerlo en el clustering. 
```{r}


# https://stackoverflow.com/questions/40003028/extracting-unique-values-from-data-frame-using-r
nombres_farmacos <- unique(datos_train[,1])

print(nombres_farmacos[1:10])

# Creamos una matriz con tantas filas como fármacos, y columnas como datos queramos utilizar. En este caso son 3 columnas porque necesitamos guardar la info de "rating", "sideEffectNumber" y "effectivenessNumber".
datos_procesados <- matrix(ncol=3, nrow=length(nombres_farmacos))

for(i in 0:length(nombres_farmacos)){
  #https://stackoverflow.com/questions/24831580/return-row-of-data-frame-based-on-value-in-a-column-r
  filas_farmaco <- datos_train2[which(datos_train$urlDrugName == nombres_farmacos[i]),]
  if (nrow(filas_farmaco)>20 ){
  mean_rating <- mean(filas_farmaco$rating)
  mean_side_effect <- mean(filas_farmaco$sideEffectsNumber)
  mean_effectiveness <- mean(filas_farmaco$effectivenessNumber)}

  #print(mean_rating)
  datos_procesados[i,] <- c(mean_rating, round(mean_side_effect)*(-1), round(mean_effectiveness))
  
}


data_train_procesado <- data.frame(datos_procesados)
rownames(data_train_procesado) <- nombres_farmacos
colnames(data_train_procesado) <- c("rating", "sideEffectNumber", "effectivenessNumber")



res_clustering <- kmeans(data_train_procesado, 5)





```

```{r}
fviz_cluster(object = res_clustering, data = data_train_procesado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```


Vamos a intentar hacer predict...... lloran2
```{r}

nombres_farmacos <- unique(datos_test[,1])

print(nombres_farmacos[1:10])

# Creamos una matriz con tantas filas como fármacos, y columnas como datos queramos utilizar. En este caso son 3 columnas porque necesitamos guardar la info de "rating", "sideEffectNumber" y "effectivenessNumber".
datos_procesados <- matrix(ncol=3, nrow=length(nombres_farmacos))

for(i in 0:length(nombres_farmacos)){
  #https://stackoverflow.com/questions/24831580/return-row-of-data-frame-based-on-value-in-a-column-r
  filas_farmaco <- datos_test2[which(datos_test$urlDrugName == nombres_farmacos[i]),]
  
  mean_rating <- mean(filas_farmaco$rating)
  mean_side_effect <- mean(filas_farmaco$sideEffectsNumber)
  mean_effectiveness <- mean(filas_farmaco$effectivenessNumber)

  #print(mean_rating)
  datos_procesados[i,] <- c(mean_rating, round(mean_side_effect)*(-1), round(mean_effectiveness))
  
}


data_test_procesado <- data.frame(datos_procesados)
rownames(data_test_procesado) <- nombres_farmacos
colnames(data_test_procesado) <- c("rating", "sideEffectNumber", "effectivenessNumber")


test_clustering = kmeans(data_test_procesado,centers=res_clustering$centers)
fviz_cluster(object = test_clustering, data = data_test_procesado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")



```

```{r}


# predict.kmeans <- function(object, newdata){
#     centers <- object$centers
#     n_centers <- nrow(centers)
#     dist_mat <- as.matrix(dist(rbind(centers, newdata)))
#     dist_mat <- dist_mat[-seq(n_centers), seq(n_centers)]
#     max.col(-dist_mat)
# }



test_preds <- predict(res_clustering, data_test_procesado)


table(test_preds)

datos_test_carlos = cbind(test_preds)

rownames(datos_test_carlos) = rownames(data_test_procesado)

test_carlos = res_clustering
test_carlos$cluster = datos_test_carlos

fviz_cluster(object = test_carlos, data = data_test_procesado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, labelsize = 0) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")



```




https://stats.stackexchange.com/questions/144616/comparing-k-means-results-to-original-data-how-to-interpret-the-resulting-plots quiza sirve para pintar graficos de estos de cajas chulis


```{r}

```

ahora voy a buscar un farmaco que exista en los dos conjuntos

```{r}
# Primero buscamos los fármacos que estén en tanto en train como en test, pq son con los que podemos ver si funciona o no el clustering. Vamos a ver si coinciden. 
x = intersect(datos_test$urlDrugName, datos_train$urlDrugName)
head(x)

# df[match(item,df$cname),]
indice_train = match('sarafem',datos_train$urlDrugName)
indice_test = match('sarafem',datos_test$urlDrugName)

# imprimimos cluster que tiene en train 
res_clustering$cluster[indice_train]

# imprimimos cluster que tiene en test
test_clustering$cluster[indice_test]

count = 0
count_na = 0
for(farmaco in x){
  indice_train = match(farmaco,datos_train$urlDrugName)
  indice_test = match(farmaco,datos_test$urlDrugName)
  
  # imprimimos cluster que tiene en train 
  c_train = res_clustering$cluster[indice_train]

  # imprimimos cluster que tiene en test
  c_test = test_clustering$cluster[indice_test]
  
  if ( !is.na(c_train) && !is.na(c_test) && c_train == c_test){
    count = count +1
  }
  
  if (is.na(c_train) || is.na(c_test))
    count_na = count_na +1
}

print(count)
```

A ver, aquí hay que repasar un poquito esto, porque, con el modelo de RandomForest sale que el errorn
```{r}
library("randomForest")


output.forest <- randomForest(as.factor(effectivenessNumber) ~ . , data = data_train_procesado, replace = T,ntree=400)

output.forest

plot(output.forest)

# Predicciones
mod_rf = predict(output.forest, newdata = data_test_procesado[-3], type="response")

# Fallo
y.falladas = sum(mod_rf!=data_test_procesado$effectivenessNumber)/length(data_test_procesado$effectivenessNumber)

print("Error en test:")
print(y.falladas)

```
Segun esto, nos podemos hcer una funcion que te diga en qué rango encontramos el mejor resultado, y con 150 árboles tenemos más que suficiente para alcanzar un 35% de error. Este error en nuestro caso es muy bueno porque tenemos muchisima subjetividad. Para unas personas, poca efectividad tendrá asociado rating 1 y para otras puede ser rating 3. Esto nos afecta mucho los resultados, y por tanto, aumenta el error. 
```{r}

valores_arboles_150 = seq(from=1, to=150, by=1)
valores_arboles_35 = seq(from=1, to=35, by=1)
valores_arboles_15 = seq(from=1, to=15, by=1)
valores_arboles_20 = seq(from=1, to=20, by=1)
valores_arboles_400 = seq(from=1, to=400, by=1)
valores_arboles_500 = seq(from=1, to=500, by=1)

obtener_arboles_optimo = function(x){
  #error_min = 100.0
  arboles_optimo = 1;
  
  for(i in x){
    error_min = 100.0
    set.seed(3)
    
    output.forest <- randomForest(as.factor(effectivenessNumber) ~ . , data = data_train_procesado, replace = T,ntree=i)

    
    # Predicciones
    mod_rf = predict(output.forest, newdata = data_test_procesado[-3], type="response")
    
    # Fallo
    y.falladas = sum(mod_rf!=data_test_procesado$effectivenessNumber)/length(data_test_procesado$effectivenessNumber)
    
    
    if(y.falladas < error_min){
      error_min = y.falladas
      arboles_optimo = i
    }
  }
  
  cat("Numero de árboles óptimo",arboles_optimo, "\n")
  
  cat("Error minimo conseguido",error_min, "\n")
  
}


cat("El error minimo con 150 árboles:\n")
## El error minimo con 150 árboles:
obtener_arboles_optimo(x = valores_arboles_150)


cat("El error minimo con 20 árboles:\n")
## El error minimo con 20 árboles:
obtener_arboles_optimo(x = valores_arboles_20)

cat("El error minimo con 15 árboles:\n")
## El error minimo con 15 árboles:
obtener_arboles_optimo(x = valores_arboles_15)

cat("El error minimo con 400 árboles:\n")
## El error minimo con 400 árboles:
obtener_arboles_optimo(x = valores_arboles_400)


cat("El error minimo con 500 árboles:\n")
## El error minimo con 500 árboles:
obtener_arboles_optimo(x = valores_arboles_500)
```



## Hierarchical clustering

Hierarchical clustering es una alternativa a los métodos de partitioning clustering que no requiere que se pre-especifique el número de clusters. Se pueden seguir dos estrategias, aglomerativa o divisiva, en este caso vamos a usar la primera.

En el método aglomerativo el agrupamiento se inicia en el base del árbol, siendo cada observación un cluster individual. Entonces los cluster se van conbinando conforma le estructura crece hasta unirse en única rama central.

```{r}
# Verificar el árbol

datos <- scale(datos_luis)

# Matriz de distancias euclídeas
mat_dist <- dist(x = datos, method = "euclidean")
# Dendrogramas con linkage complete y average
hc_euclidea_complete <- hclust(d = mat_dist, method = "complete")
hc_euclidea_average  <- hclust(d = mat_dist, method = "average")

cor(x = mat_dist, cophenetic(hc_euclidea_complete))
cor(x = mat_dist, cophenetic(hc_euclidea_average))

```



```{r}
# Cortar el árbol para generar los clusters

library(factoextra)
datos <- scale(datos_luis)
set.seed(101)

hc_euclidea_completo <- hclust(d = dist(x = datos, method = "euclidean"),
                               method = "complete")

fviz_dend(x = hc_euclidea_completo, k = 2, cex = 0.6) +
  geom_hline(yintercept = 5.5, linetype = "dashed") +
  labs(title = "Herarchical clustering",
       subtitle = "Distancia euclídea, Lincage complete, K=2")
```

```{r}
fviz_cluster(object = list(data=datos, cluster=cutree(hc_euclidea_completo, k=4)),
             ellipse.type = "convex", repel = TRUE, show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "Hierarchical clustering + Proyección PCA",
       subtitle = "Distancia euclídea, Lincage complete, K=4") +
  theme_bw() +
  theme(legend.position = "bottom")
```


```{r}
set.seed(101)
# Se simulan datos aleatorios con dos dimensiones
datos <- matrix(rnorm(n = 100*2), nrow = 100, ncol = 2, dimnames = list(NULL,c("x", "y")))
datos <- as.data.frame(datos)

# Se determina la media que va a tener cada grupo en cada una de las dos
# dimensiones. En total 2*4 medias. Este valor se utiliza para separar
# cada grupo de los demás.
media_grupos <- matrix(rnorm(n = 8, mean = 0, sd = 4), nrow = 4, ncol = 2,
                       dimnames = list(NULL, c("media_x", "media_y")))
media_grupos <- as.data.frame(media_grupos)
media_grupos <- media_grupos %>% mutate(grupo = c("a","b","c","d"))

# Se genera un vector que asigne aleatoriamente cada observación a uno de
# los 4 grupos
datos <- datos %>% mutate(grupo = sample(x = c("a","b","c","d"), size = 100, replace = TRUE))

# Se incrementa el valor de cada observación con la media correspondiente al
# grupo asignado.
library(dplyr)
datos <- left_join(datos, media_grupos, by = "grupo")
datos <- datos %>% mutate(x = x + media_x,
                          y = y + media_y)
datos <- datos %>% select(grupo, x, y)
ggplot(data = datos, aes(x = x, y = y, color = grupo)) +
  geom_point(size = 2.5) +
  theme_bw()
```



```{r}
# Se calculan las distancias
matriz_distancias <- dist(x = datos[, c("x", "y")], method = "euclidean")
set.seed(567)
hc_euclidea_completo <- hclust(d = matriz_distancias, method = "complete")
hc_euclidea_single   <- hclust(d = matriz_distancias, method = "single")
hc_euclidea_average  <- hclust(d = matriz_distancias, method = "average")
```


```{r}
par(mfrow = c(3,1))
plot(x = hc_euclidea_completo, cex = 0.6, xlab = "", ylab = "", sub = "",
     main = "Distancia euclídea, Linkage complete")
plot(x = hc_euclidea_single, cex = 0.6, xlab = "", ylab = "", sub = "",
     main = "Distancia euclídea, Linkage single")
plot(x = hc_euclidea_average, cex = 0.6, xlab = "", ylab = "", sub = "",
     main = "Distancia euclídea, Linkage average")
```
```{r}
plot(x = hc_euclidea_completo, cex = 0.6, sub = "",
     main = "Distancia euclídea, Linkage complete, k=4",
     xlab = "", ylab = "", labels = datos[, "grupo"])
abline(h = 6, lty = 2)

table(cutree(hc_euclidea_completo, h = 6), datos[, "grupo"])
```

































