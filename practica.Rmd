---
title:
author:
- "Alejandro Campoy Nieves"
- "Gema Correa Fernández"
- "Luis Gallego Quero"
- "Jonathan Martín Valera"
- "Andrea Morales Garzón"
date: "14 de noviembre de 2018"
output:
  pdf_document:
    keep_tex: true
lang: es-ES
geometry: margin=1in
header-includes:
  - \usepackage{fancyhdr}
  - \fancyfoot[CO,CE]{My footer}
---

<!----------------------------------------------------------------Portada------------------------------------------------------------------>
####################
\thispagestyle{empty}

\begin{center} \huge \textbf{Tratamiento Inteligente de datos} \end{center}
\vspace{0.3cm}
\begin{center} \huge \textbf{(TID)} \end{center}
\vspace{1.7cm}
\begin{center} \Large \textbf{\textsc{Prácticas de la asignatura}} \end{center}
\vspace{0.2cm}
\begin{center} \large \textbf{2018-2019} \end{center}

\vspace{2.5cm}

\textbf{\large En colaboración con:}
\vspace{0.2cm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/logoUGR.jpg}
    \label{imagen2}
\end{figure}

\vspace{2.5cm}

\hspace{8.5cm}{\large \textbf{Participantes}}

\vspace{0.25cm}

\hspace{8.5cm}{Alejandro Campoy Nieves:  \href{mailto:alejandroac79@correo.ugr.es}{\textcolor{blue}{\underline{alejandroac79@correo.ugr.es}}}}

\vspace{0.15cm}

\hspace{8.5cm}{Gema Correa Fernández:  \href{mailto:gecorrea@correo.ugr.es}{\textcolor{blue}{\underline{gecorrea@correo.ugr.es}}} }

\vspace{0.15cm}

\hspace{8.5cm}{Luis Gallego Quero:  \href{mailto:lgaq94@correo.ugr.es}{\textcolor{blue}{\underline{lgaq94@correo.ugr.es}}} }

\vspace{0.15cm}

\hspace{8.5cm}{Jonathan Mart?n Valera:  \href{mailto:jmv742@correo.ugr.es}{\textcolor{blue}{\underline{jmv742@correo.ugr.es}}} }

\vspace{0.15cm}

\hspace{8.5cm}{Andrea Morales Garzón:  \href{mailto:andreamgmg@correo.ugr.es}{\textcolor{blue}{\underline{andreamgmg@correo.ugr.es}}} }

\vspace{0.15cm}

\newpage

<!----------------------------------------------------------------Indices------------------------------------------------------------------>
####################
\thispagestyle{empty}
\tableofcontents
\newpage

\thispagestyle{empty}
\listoffigures
\newpage

\thispagestyle{empty}
\listoftables
\newpage

\pagestyle{fancy}
\fancyhf{}
\lhead{Proyecto: Técnicas aplicadas para análisis inteligente de datos}
\rhead{\thepage}
\setcounter{page}{1}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(3) # semilla para obtención de los mismos resultados
getwd()     # para saber el directorio de trabajo
```

<!-----------------------------------------1. Comprender el problema a resolver------------------------------------------------------>

<!--------------------------------------------1. Comprender el problema a resolver--------------------------------------------------------->
# 1. Comprender el problema a resolver

Para la realización y aplicación de las técnicas explicadas a lo largo del curso, hemos seleccionado un _dataset_ proporcionado por [*UCI Machine Learning Repository*](https://archive.ics.uci.edu/ml/index.php). En concreto, hemos escogido [**Drug Review Dataset**](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Druglib.com%29), una exhaustiva base de datos de medicamentos organizada por relevancia para medicamentos específicos. El conjunto de datos proporciona revisiones de pacientes sobre medicamentos específicos junto con las condiciones relacionadas. Además, las revisiones se agrupan en informes sobre tres aspectos: beneficios, efectos secundarios y comentarios generales. De igual modo, las calificaciones están disponibles con respecto a la satisfacción general, así como una calificación de efectos secundarios y de eficacia de 5 pasos. Los datos se obtuvieron rastreando los sitios de revisión farmacéutica en línea.



El objetivo principal del estudio es:

 - Realizar un análisis de sentimientos en relación con la experiencia en el uso de dichos medicamentos, como por ejemplo la efectividad, efectos secundarios...
 
 - Compatibilizar dicho modelo de datos con otros conjuntos de datos aportados en: [**Drugs.com**](https://www.drugs.com/)
 
En este proyecto nos centraremos en el **análisis y experiencia de los usuarios** en el uso de los distintos medicamentos.

Las características de este conjunto de datos vienen descritas en la siguiente tabla:

<!--- Tabla --->

| DataSet Characteristics:   | Multivariate, Text                     | Number of Instances:  | 4143 | Area:               | N/A        |
|----------------------------|----------------------------------------|-----------------------|------|---------------------|------------|
| Attribute Characteristics: | Integer                                | Number of Attributes: | 8    | Date Donated        | 2018-10-02 |
|----------------------------|----------------------------------------|-----------------------|------|---------------------|------------|
| Associated Tasks:          | Classification, Regression, Clustering | Missing Values?       | N/A  | Number of Web Hits: | 7001       |

Los datos se dividen en un conjunto train (75%) y otro conjunto test (25%) y se almacenan en dos archivos.tsv (tab-separated-values), respectivamente. Los atributos que tenemos en este dataset son:

1. **urlDrugName** (categorical): nombre de la droga
2. **condition** (categorical): nombre de la condición
3. **benefitsReview** (text): opinión del paciente sobre los beneficios
4. **sideEffectsReview** (text): opinión del paciente sobre los efectos secundarios
5. **commentsReview** (text): comentario general del paciente
6. **rating** (numerical): clasificación de paciente de 10 estrellas
7. **sideEffects** (categorical): clasificación de 5 pasos de efectos secundarios
8. **effectiveness** (categorical): clasificación de efectividad de 5 pasos


<!--------------------------------------------2. Prepocesamiento de datos------------------------------------------------------------>

# 2. Prepocesamiento de datos

Para poder analizar el dataset y realizar el prepocesamiento al mismo, lo primero que se va hacer es leer tanto el conjunto de datos train como de test. Primero, leeremos los datos con los que se va a entrenar y luego los datos test.

## 2.1. Lectura de datos

A continuación, leemos nuestro dataset train y test:

```{r lectura }
# Lectura de datos train
datos_train <- read.table("datos/drugLibTrain_raw.tsv", sep="\t", comment.char="",
                          quote = "\"", header=TRUE)
head(datos_train, 5) # visualizar las 5 primeras filas
summary(datos_train) # información sobre los datos
View(datos_train)    # vista de la tabla

# Lectura de datos test
datos_test <- read.table("./datos/drugLibTest_raw.tsv", sep="\t", comment.char="",
                         quote = "\"", header=TRUE)
head(datos_test, 5) # visualizar las 5 primeras filas
summary(datos_test) # información sobre los datos
View(datos_test)    # vista de la tabla
```

## 2.2. Falta de datos, categorización, normalización, reducción de dimensionalidad.

<!--- Borrar si añgún atributo no nos da información como un ID, hacer una transformación con los atributos asimétricos, ya que son necesarios para la aplicación de algunos métodos de aprendizaje sensibles a distancias. Se consideran asimétricos cuando el valor skewness se aleja de 0, podemos eliminar las variables con varianza 0 o muy próximas. --->

# Procesar datos

## Eliminar columna del ID.
Como ya se ha comentado previamente, al conjunto de datos utilizado se le ha añadido de forma automática una novena columna, que representa un ID para cada uno de los datos con los que estamos trabajando. Como no nos aporta información, hemos decidido quitarla directamente del dataframe. Esta columna se corresponde con la primera columna del dataframe, por lo cuál, debemos eliminar la columna que se corresponde con la posición 1. Los cambios que hacemos en el dataset deben modificarse tanto en el conjunto de test como el de train, para que los resultados sean consistentes.
```{r}
datos_train = datos_train[-1]
datos_test = datos_test[-1]


```

Una vez eliminada la columna anterior, ya podemos continuar con el procesamiento de los datos. 

Primero tenemos que usar la librería que procese los datos de tipo texto en R. La más conocida se llama \textbf{tm}. Si no la tenemos instalada en R, tnemos que hacerlo previamente. Para eso hacemos lo siguiente:
```{r}


# install.packages("tm")
# install.packages("SnowballC")

library("tm")
library("SnowballC")
```

## Creación del corpus

Para poder obtener la estructura con la que vamos a procesar nuestra información, debemos obtener un vector con documentos. En nuestro caso, cada uno de los documentos se corresponde con una opinión sobre un fármaco. Para ello, primero debemos de construir un vector con todas los opiniones del dataframe.

Sólo nos faltaría convertir cada elemento del vector al formato de documento. Podemos usar la función VectorSource para hacer esta conversión.

```{r}

# Nos quedamos con la única columna del dataset que nos interesa. Necesitamos obtenerla en forma de vector, y no como un dataframe de una columna, por lo que usamos as.vector para hacer la conversión
benefits_review_data = as.vector(datos_train$benefitsReview)

# Lo convertimos en la estructura de documento, y lo guardamos ya en el corpus que lo vamos a utilizar.
corpus = (VectorSource(benefits_review_data))

# Creamos el propio corpus
corpus <- Corpus(corpus)
#summary(corpus)

``` 

Podemos ver que funciona accediendo a uno cualquiera, por ejemplo, vamos a acceder al cuarto comentario almacenado. 
```{r}

# P Si nos fijamos en el contenido, vemos que tiene signos de puntuación y exclamación.
inspect(corpus[4])
corpus[[4]]$content
```

## Eliminar signos de puntuación. 

Como hemos podido ver en el documento que se ha mostrado por pantalla, en él se aprecia el uso de signos de puntuación y exclamación. En un principio, no tiene sentido en \textit{Data Mining}contemplar los signos de puntuación, ya que no nos van a aportar información. Por ello, los quitamos, como se puede ver a continuación.

```{r}
# 4. Una vez que tenemos el corpus creado, continuamos con el procesamiento. 
corpus <- tm_map(corpus, content_transformer(removePunctuation))

```
Si volvemos a mostrar la opinión número cuatro, vemos como todos los signos han desaparecido. De hecho, podemos inspeccionar el corpus, y se ve como todos los signos de puntuación, exclamación y derivados ya no están.

```{r}
inspect(corpus[4])

```

## Stopwords. 

En cualquier idioma, hay palabras tan tan tan comunes que no nos aportan información relevante. Por ejemplo, en español, las palabras "la", "a", "en", "de" son ejemplos de lo que se conoce como \textit{stopwords}. Este tipo de palabras debemos de suprimirlas de nuestro corpus. Como el contenido del corpus está en inglés, debemos especificar el idioma correcto para que nos elimine del corpus las palabras adecuadas en dicho idioma.

```{r}
corpus <- tm_map(corpus, content_transformer(removeWords), 
          stopwords("english"))

```

Sin embargo, tenemos otro problema, y es que no todas las \textit{stopwords} se han borrado. Sí se han borrado todas las \textit{stopwords} cuyas letras que la componen están en minúscula, pero no si una de las letras están en mayúscula. Por ejemplo: todas las "the" se han borrado, pero no las "The". Esto nos obliga a tener que pasar todos los caracteres del texto a letras minúscula, y a repetir de nuevo el proceso que elimina las \textit{stopwords}. 

```{r}

corpus <- tm_map(corpus, content_transformer(tolower))
inspect(corpus[1])
corpus <- tm_map(corpus, content_transformer(removeWords),stopwords("english"))

```

Ahora ya hemos eliminado las stopwords de forma correcta.


## Stemming
El siguiente paso consiste en reducir el número de palabras totales con las que estamos trabajando. En este caso, se trata de reducir aquellas que no nos aportan nada relevante a lo que ya tenemos. En la columna con la que estamos trabajando en este dataframe, se repite una gran cantidad de veces la palabra "benefit", al igual que "benefits". 

Sin embargo, realizar el análisis de nuestros datos con ambas palabras no tiene gran relevancia, ya que una no aporta nada respecto a la otra. Este es un ejemplo del tipo de casos que se nos dan en nuestro dataset; por ejemplo. Igual ocurre con "reduce" y "reduced", por ejemplo. Este tipo de situaciones son las que intentamos corregir con este paso. 

Vamos a ver un ejemplo de este suceso, que se da por ejemplo en los siguientes valores del corpus (y en muchos más). 

```{r}
inspect(corpus[183])
inspect(corpus[213])


```

A continuación, aplicamos el proceso de stemming mediante la siguiente orden:

```{r}
corpus <- tm_map(corpus, stemDocument)
```

Si ahora volvemos a mostrar el contenido de dichas opiniones, podemos ver que el stemming se ha hecho efectivo: donde ponía \textit{benefits}, ahora pone \textit{benefit}, como se puede comprobar si volvemos a mostrar dichos elementos del corpus. De hecho, si nos fijamos, no solo esta palabra ha resultado modificada, sino que se han resumido muchas más palabras en comparación a como teníamos los documentos en el momento previo a la aplicación del método \textit{Stem}. Desde este momento, ya tenemos nuestro conjunto reducido a nivel de concepto.

```{r}
inspect(corpus[183])
inspect(corpus[213])
```

## Borrar espacios en blanco innecesarios
Hasta el momento hemos hecho distintos cambios en el texto de nuestro dataset. No solo hemos modificado algunas palabras, sino que también hemos borrado otras muchas. Por ello, es adecuado asegurarnos que no hay más espacios en blanco que los que separan las palabras del texto. Para asegurarnos de ello, podemos ejecutar la siguiente orden, que se encarga de suprimir los espacios en blanco sobrantes.

```{r}
corpus <- tm_map(corpus, stripWhitespace) 
```



